{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F-One Responses Quality Evaluation with UniEval Metric\n",
    "\n",
    "The quality of F-One responses was assessed through UniEval metrics. Specifically, we used UniEval to evaluate \n",
    "the chatbot responses associated with the Information-Retrieval system related to the FIA Formula 1 regulations. \n",
    "\n",
    "### UniEval paper:\n",
    "\n",
    "*[Towards a Unified Multi-Dimensional Evaluator for Text Generation](https://arxiv.org/abs/2210.07197)*\n",
    "\n",
    "### Unieval GitHub Repository:\n",
    "\n",
    "https://github.com/maszhongming/UniEval.git\n",
    "\n",
    "\n",
    "### Environment\n",
    "```\n",
    "git clone https://github.com/maszhongming/UniEval.git\n",
    "cd UniEval\n",
    "pip install -r requirements.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Phase\n",
    "\n",
    "In this phase we used UniEval to obtain the Factual Consistency Score.\n",
    "The consistency score has a value between [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import convert_to_json\n",
    "from metric.evaluator import get_evaluator\n",
    "\n",
    "\n",
    "task = 'fact'\n",
    "\n",
    "# a list of source documents\n",
    "src_list = [\"\"\"The term \"Engineering Trailer\" refers to a branded temporary standalone structure that is brought into the paddock by an F1 Team. It includes any irremovable fixtures, fittings, and equipment integrated into the structure. The purpose of the Engineering Trailer is to provide a working environment for engineering activities during a Competition or Testing of Current Cars. It's important to note that the definition excludes any structures, fixtures, fittings, or equipment that are constructed or installed into permanent or existing paddock buildings, such as the pit garages.\n",
    " \n",
    "\"\"\"]\n",
    "# a list of model outputs (claims) to be evaluataed\n",
    "output_list = [\"\"\"\"Engineering Trailer\" means a branded temporary standalone structure, and any irremovable fixtures, fittings and equipment integrated into such structure that is brought into the paddock and constructed by an F1 Team to provide a working environment for engineering purposes during a Competition or Testing of Current Cars.\n",
    "\n",
    "\"\"\"]\n",
    "\n",
    "# Prepare data for pre-trained evaluators\n",
    "data = convert_to_json(output_list=output_list, src_list=src_list)\n",
    "# Initialize evaluator for a specific task\n",
    "evaluator = get_evaluator(task)\n",
    "# Get factual consistency scores\n",
    "eval_scores = evaluator.evaluate(data, print_result=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "OUTPUT:\n",
    "\n",
    "Evaluation scores are shown below:\n",
    "+-------------+----------+\n",
    "|  Dimensions |  Score   |\n",
    "+-------------+----------+\n",
    "| consistency | 0.756445 |\n",
    "+-------------+----------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Phase\n",
    "\n",
    "In this phase we used UniEval to evaluate the response generated by F-One at the dialogue level. In this case, unlike the previous evaluation phase, we evaluated the following parameters using UNIEVAL: \n",
    "- Naturalness\n",
    "- Coherence\n",
    "- Engagingness\n",
    "- Groundedness\n",
    "- Understandability \n",
    "\n",
    "Engagingness is the only dimension that uses summation scores, as it indicates the total volume of interesting facts \n",
    "presented in the response. Therefore, the scoring range for engagingness is [0, +âˆž), while all others are [0, 1].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import convert_to_json\n",
    "from metric.evaluator import get_evaluator\n",
    "\n",
    "task = 'dialogue'\n",
    "\n",
    "# a list of dialogue histories\n",
    "src_list = [\"\"\"What is the number of rear lights that all cars must have operating for the duration of the competition and which are their features?\n",
    "\"\"\"]\n",
    "# a list of additional context that should be included into the generated response\n",
    "context_list = [\"\"\"All cars must have three rear lights in working order throughout the Competition which: have been supplied by FIA designated manufacturers, are clearly visible from the rear, can be switched on by the driver when seated normally in the car.\n",
    "\n",
    "\"\"\"]\n",
    "# a list of model outputs to be evaluated\n",
    "output_list = [\"\"\"All cars must have three rear lights in working order throughout the competition. These lights must be supplied by FIA designated manufacturers, clearly visible from the rear, and can be switched on by the driver when seated normally in the car. Additionally, the lights described in Article 14.3 of the Technical Regulations must be illuminated at all times when using intermediate or wet-weather tyres.\n",
    "\"\"\"]\n",
    "\n",
    "# Prepare data for pre-trained evaluators\n",
    "data = convert_to_json(output_list=output_list, \n",
    "                       src_list=src_list, context_list=context_list)\n",
    "# Initialize evaluator for a specific task\n",
    "evaluator = get_evaluator(task)\n",
    "# Get multi-dimensional evaluation scores\n",
    "eval_scores = evaluator.evaluate(data, print_result=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "OUTPUT:\n",
    "\n",
    "Evaluation scores are shown below:\n",
    "+-------------------+----------+\n",
    "|     Dimensions    |  Score   |\n",
    "+-------------------+----------+\n",
    "|    naturalness    | 0.999625 |\n",
    "|     coherence     | 0.999819 |\n",
    "|    engagingness   | 2.998616 |\n",
    "|    groundedness   | 0.998462 |\n",
    "| understandability | 0.999607 |\n",
    "|      overall      | 1.399226 |\n",
    "+-------------------+----------+\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
